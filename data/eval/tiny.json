[
    {
        "title": "How many reference points do we need for unambiguously locating other points in a space?",
        "question": "Let us suppose a metric spaces S with N dimensions. And let us suppose that we have a distince function d, which is able to measure the distance by any pair of point in S. \r\n\r\nNow, let us consider that I only have a set of D distance measures between a point P and a set R of reference points. How many points the set R shoud have in order to unambiguasly locate the point P?\r\n\r\nIntuitively, I would say that N points would be enough; one for each dimension. Am I correct?",
        "answer": "Roughly speaking you need one more to resolve ambiguity.  If you think about $N=2$, you have two points in $R$ and the distances from them to $P$.  You draw circles around the two points in $R$ with the appropriate radii.  Those circles intersect in two points (unless they are tangent) and you need one more distance to resolve the ambiguity.  The same thing happens in more dimensions.  If you have the same number of points as dimensions, you draw a sphere around each point and there is a finite number of intersections.  Unless you are unlucky, one more point and set of distances will resolve the question."
    },
    {
        "title": "Continuity counterexamples for probability measures",
        "question": "I&#39;m given a measurable space $(\\Omega, \\mathcal{F})$ and a finitely additive measure $m$ that satisfies $m(\\Omega) = 1$. I&#39;m asked to give counterexamples for continuity from above and below.\r\n\r\nMy attempt (discontinuity from above): take $\\Omega = \\mathbb{N}$ and $\\mathcal{F} = \\mathcal{P}(\\mathbb{N})$ the set of subsets of $\\mathbb{N}$. Define:\r\n$$\r\nm(A) = \\begin{cases}\r\n   0, &amp; A \\text{ finite} \\\\\r\n   1, &amp; \\text{otherwise}\r\n\\end{cases}\r\n$$\r\nNow, take $A_n = \\{ j \\in \\mathbb{N} \\ |\\ j \\geq n \\} $ which satisfies\r\n$$\r\nA_1 \\supset A_2 \\supset \\dots, \\quad \\bigcap_{i=1}^{\\infty} A_i = \\emptyset \\quad (A_n \\downarrow \\emptyset)\r\n$$\r\nThen, it obviously holds\r\n\\begin{align*}\r\n  m\\left(\\bigcap_i^{\\infty} A_i\\right) &amp;= m(\\emptyset) = 0  \\\\\r\n  \\lim_{n \\to \\infty} m(A_n) &amp;= \\lim_{n \\to \\infty} m\\left( \\{ j \\geq n, j \\in \\mathbb{N} \\} \\right) = \\lim_{n \\to \\infty} 1 = 1 \\neq\r\n  m\\left( \\bigcap_i^{\\infty} A_i \\right)\r\n\\end{align*}\r\nsince, $\\forall n \\geq 1$, $A_n$ is a co-finite set. Is my approach correct?",
        "answer": "No.\r\nActually, your finite additive measure $m$ is not well defined.\r\n\r\nFor example: by additivity $m(3n+1)+m(3n+2) = m(3n+1\\cup3n+2)$ which imply $1+1 = 1$ which is false.\r\n"
    },
    {
        "title": "Independent with a vector v.s. independent with its components",
        "question": "Suppose that $Z$ (a scalar) is independent with $X=(X_1,\\ldots,X_n)$, $n&gt;1$. Then, $Z$ is independent with $X_1,\\ldots,X_n$ because each of the latter is a function of $X$.\r\n\r\nI suspect the reverse direction: $Z$ being independent with $X_1,\\ldots,X_n$ implying $Z$ being independent with $X$ is **not** true. Most likely because we would need some info about the joint distribution of $X_1,\\ldots,X_n$. But I can&#39;t think of a counter example. So could you please provide one as well as some intuition on how you arrive at it?",
        "answer": "Yes, in general the converse is not correct. The reason is, essentially, that pairwise independence does not imply independence.\r\n\r\n**Example** Let $\\Omega := \\{0,1\\}^2$ and $\\mathbb{P}(\\{\\omega\\}) := 1/4$, $\\omega \\in \\Omega$. For $\\omega = (\\omega_1,\\omega_2)$ define\r\n\r\n$$X_1(\\omega) := \\omega_1 \\qquad X_2(\\omega) := \\omega_2 \\qquad Z(\\omega) :=1_{\\{\\omega_1=\\omega_2\\}}.$$\r\n\r\nThen it is not difficult to see that\r\n\r\n$$ \\mathbb{P}(X_1 = i, Z=j) = \\frac{1}{4} = \\mathbb{P}(X_1=i) \\mathbb{P}(Z=j)$$\r\n\r\nfor any $i,j \\in \\{0,1\\}$ which shows that $X_1$ and $Z$ are independent. In an analogous way we find that $X_2$ and $Z$ are independent. However, $Z$ and $X:=(X_1,X_2)$ are not independent since\r\n\r\n$$\\mathbb{P}(Z=1, X=(1,1)) = \\frac{1}{4} \\neq \\frac{1}{8} = \\mathbb{P}(Z=1) \\mathbb{P}(X=(1,1)).$$\r\n\r\n\r\n"
    },
    {
        "title": "If the $(n-1)$th moment exists does the $n$th moment necessarily exist?",
        "question": "Let&#39;s suppose the distribution is unknown but that the second moment is known to be finite. Doesn&#39;t this imply that the distribution should fall off exponentially fast and therefore higher moments must also exist?",
        "answer": "No, not at all. The decay is exponential if exponential moments exist, but we cannot expect exponential decay if the second moment exists. Just consider e.g. a random variabl $X$ with density\r\n\r\n$$f(x) := c \\frac{1}{x^4} 1_{(1,\\infty)}(x)$$\r\n\r\n(where $c&gt;0$ is chosen such that $\\int f(x) \\, dx =1$). Then the random variable has second moments, but $\\mathbb{E}(|X|^3)= \\infty$."
    },
    {
        "title": "Relation between convergence in distribution and in probability",
        "question": "Does convergence in distribution imply convergence in probability ? \r\n\r\nI suppose no, but I need a counterexample. Does anyone know any counterexamples ?",
        "answer": "Consider the probability space $((0,1),\\mathcal{B}(0,1))$ endowed with the Lebesgue measure  $\\lambda$ and the random variables $$X(\\omega) := 1_{(0,1/2)}(\\omega) \\qquad \\qquad Y(\\omega) := 1_{(1/2,1)}(\\omega), \\qquad \\omega \\in (0,1).$$ Then $X \\sim Y$. Set $X_n(\\omega) := Y(\\omega)$ for all $n \\in \\mathbb{N}, \\omega \\in (0,1)$.\r\n\r\n1.  $X_n \\to X$ in distribution since $X_n \\sim X$ for any $n \\in \\mathbb{N}$\r\n2. $X_n$ does not converge in probability to $X$ since $$\\lambda(|X_n-X|&gt;1/2)= \\lambda(|X-Y|&gt;1/2) = 1.$$"
    },
    {
        "title": "How to convert a problem to a stars and bars problem?",
        "question": "Continued question from [here][1].\r\n\r\n\r\n----------\r\nWith certain questions I have $x_i$ being constrained by various different inequalities, I want to know how to remove these from the problem, to bring me back to a straight forward application of the stars and bars method.\r\n\r\nHow can I convert a problem like:\r\n\r\n$$\\def\\x{x_}\\x1+\\x2+\\x3+\\dots+x_i =15$$ with $\\x1\\leq3$\r\n\r\nBack to a simple stars and bars problem such as $$y_1+\\x2+\\x3+\\dots+x_i=33$$\r\n\r\nHow can I convert that bad $\\x1$ into a nice $y_1$\r\n\r\n$x_i,y_i\\in\\mathbb{Z^{\\geq0}}$\r\n\r\n\r\n----------\r\nI can see how to do it for $x_i \\geq k$ since I can just take $y_a =x_a - k \\geq 0$ and take $x_a = y_a +k$ and sub it into the original equation, which gives me some stars and some bars :).\r\n\r\n  [1]: https://math.stackexchange.com/questions/910809/how-to-use-stars-and-barscombinatorics",
        "answer": "I don&#39;t know if it can be done as a single S&amp;B calculation but here are two S&amp;B approaches:\r\n\r\n(1) Do S&amp;B for the equation &lt;b&gt;without&lt;/b&gt; restriction. Subtract from that another S&amp;B with restriction $x_1 \\geq 4$.\r\n\r\n(2) Do a separate S&amp;B, omitting $x_1$ from the equation, for each of the four cases: $x_1 = 0,1,2,3$. Then sum the four results.\r\n\r\n&lt;b&gt;Example&lt;/b&gt;: Take $i=4$ so we have\r\n\r\n$$\\def\\x{x_}\\x1+\\x2+\\x3+x_4 =15\\qquad\\mbox{with }\\x1\\leq3$$\r\n\r\n(1) S&amp;B without restriction: we have $4-1 = 3$ bars and $15$ stars. #Ways= $\\binom{18}{3}$.\r\n\r\nS&amp;B with $x_1 \\geq 4$: we have $4-1=3$ bars and $11$ stars. #Ways = $\\binom{14}{3}$. \r\n\r\nTotalWays = $\\binom{18}{3} - \\binom{14}{3} = 452$.\r\n\r\n(2) S&amp;B with $x_1=0$: we have $3-1=2$ bars and $15$ stars. #Ways = $\\binom{17}{2}$.\r\n\r\n(The equation we have here is: $x_2+x_3+x_4 =15$.)\r\n\r\nS&amp;B with $x_1=1$: we have $3-1=2$ bars and $14$ stars. #Ways = $\\binom{16}{2}$.\r\n\r\nS&amp;B with $x_1=2$: we have $3-1=2$ bars and $13$ stars. #Ways = $\\binom{15}{2}$.\r\n\r\nS&amp;B with $x_1=3$: we have $3-1=2$ bars and $12$ stars. #Ways = $\\binom{14}{2}$.\r\n\r\nTotalWays = $\\binom{17}{2} + \\binom{16}{2} + \\binom{15}{2} + \\binom{14}{2} = 452$."
    },
    {
        "title": "Any counter example for&#160;$P(A|C)=P(A),P(B|C)=P(B)$ but&#160;$P(A\\cap B|C)\\neq P(A\\cap B)$?",
        "question": "Let $A,B,C$ be three events, what would be an example that $P(A|C)=P(A)$ and $P(B|C)=P(B)$ **do not** imply $P(A\\cap B|C)= P(A\\cap B)$?",
        "answer": "Consider a dice with 8 faces with numbers from $1$ to $8$ on them respectively. We roll that dice. Denote $X$ as the resulting face.\r\n\r\nLet $A=[X=1,X=2]$, $B = [X=2,X = 3]$, $C=[X~\\text{is even}]$.  Then we definitely have $$P(A|C)=P(X=2\\vert C)=\\frac14=P(A)$$ and $$P(B|C)=P(X=2\\vert C)=\\frac14=P(B).$$\r\n\r\nBut $$P(A\\cap B|C) = P(X = 2\\vert C) = \\frac14\\ne \\frac18 = P(X = 2) = P(A\\cap B) $$"
    },
    {
        "title": "Independent with a vector v.s. independent with its components",
        "question": "Suppose that $Z$ (a scalar) is independent with $X=(X_1,\\ldots,X_n)$, $n&gt;1$. Then, $Z$ is independent with $X_1,\\ldots,X_n$ because each of the latter is a function of $X$.\r\n\r\nI suspect the reverse direction: $Z$ being independent with $X_1,\\ldots,X_n$ implying $Z$ being independent with $X$ is **not** true. Most likely because we would need some info about the joint distribution of $X_1,\\ldots,X_n$. But I can&#39;t think of a counter example. So could you please provide one as well as some intuition on how you arrive at it?",
        "answer": "Yes, in general the converse is not correct. The reason is, essentially, that pairwise independence does not imply independence.\r\n\r\n**Example** Let $\\Omega := \\{0,1\\}^2$ and $\\mathbb{P}(\\{\\omega\\}) := 1/4$, $\\omega \\in \\Omega$. For $\\omega = (\\omega_1,\\omega_2)$ define\r\n\r\n$$X_1(\\omega) := \\omega_1 \\qquad X_2(\\omega) := \\omega_2 \\qquad Z(\\omega) :=1_{\\{\\omega_1=\\omega_2\\}}.$$\r\n\r\nThen it is not difficult to see that\r\n\r\n$$ \\mathbb{P}(X_1 = i, Z=j) = \\frac{1}{4} = \\mathbb{P}(X_1=i) \\mathbb{P}(Z=j)$$\r\n\r\nfor any $i,j \\in \\{0,1\\}$ which shows that $X_1$ and $Z$ are independent. In an analogous way we find that $X_2$ and $Z$ are independent. However, $Z$ and $X:=(X_1,X_2)$ are not independent since\r\n\r\n$$\\mathbb{P}(Z=1, X=(1,1)) = \\frac{1}{4} \\neq \\frac{1}{8} = \\mathbb{P}(Z=1) \\mathbb{P}(X=(1,1)).$$\r\n\r\n\r\n"
    },
    {
        "title": "Counter example for Random variable",
        "question": "I read  the result that if $|X|$ is random variable then  $X$ need not be random variable.\r\n\r\nSo, I am looking for counter example. \r\n\r\nThank you for your time.",
        "answer": "Take any non-measurable subset $A\\subseteq\\mathbb{R}$ and define\r\n$$\r\nX(\\omega)=\\mathbf{1}_A(\\omega)-\\mathbf{1}_{A^c}(\\omega),\\quad\\omega\\in\\mathbb{R},\r\n$$\r\nwhere $\\mathbf{1}_A$ is the indicator function for the set $A$. Then $|X|=1$ is a random variable but $X$ isn&#39;t a random variable (why?)."
    },
    {
        "title": "A counter example about fraction of 2 sets",
        "question": "In a software company there are 2 departments, called A and B. In department A, the fraction of senior programmers (of out the junior programmers in this department) finishing the jobs in time is higher than the fraction of intern finishing the job in time (of out the interns in this apartment). \r\n\r\nSame thing happens at department B, the fraction of senior programmers finishing the jobs in time is higher than the fraction of intern finishing the job in time. \r\n\r\nis it necessary true that the fraction of senior programmers in either department A or B (out of all senior programmers in either department) is higher than the fraction of intern in either department A or B (out all of interns in either department)?\r\n\r\nProvide a counter example (number of senior programmers and interns in each department, and the fraction that finishes the job in time)\r\n\r\nREMARK: I don&#39;t really think this problem is true or cannot be solved by providing a counter example, but I may be wrong. Please give me some ideas on this problem. Thank you.",
        "answer": "This is a question demonstrating [Simpson&#39;s Paradox][1].\r\n\r\nYes, it is possible. Counter example: suppose that in department A, 81% (81 out of 100) of senior programmers finish their jobs and 80.5% (161/200) of the interns do. Suppose that in department B, 60% (120/200) of senior programmers finish their jobs and 59.5% (59/100) of interns do. \r\n\r\nOverall, this means that only 67% (201/300) of senior programmers finish their jobs, while 73.3% (220/300) of interns do, even though within each department, a greater fraction of senior programmers finish their jobs than interns.\r\n\r\nThis effect seems paradoxical. The reason it is true is because taking the &quot;overall&quot; average (combining the two departments) hides the true efficiency of the senior programmers. In departments A and B, the senior programmers are more efficient than the interns. Notably, both groups were much less efficient in department B. Most importantly, there are way more interns in department A than B, and vice-versa for the seniors. This means that the excess of seniors in department B (with lower efficiency) brings down the overall senior efficiency, while the surplus of interns in department A (with higher efficiency) brings up the interns&#39; overall efficiency. This makes it appear that the interns are more efficient, while in fact, they are not.\r\n\r\n  [1]: http://en.wikipedia.org/wiki/Simpson%27s_paradox"
    },
    {
        "title": "What is the difference and relationship between the binomial and Bernoulli distributions",
        "question": "How should I understand the difference or relationship between binomial and Bernoulli distribution",
        "answer": "A Bernoulli random variable has two possible outcomes:  $0$ or $1$.  A binomial distribution is the sum of **independent** and **identically** distributed Bernoulli random variables.\r\n\r\nSo, for example, say I have a coin, and, when tossed, the probability it lands heads is $p$.  So the probability that it lands tails is $1-p$ (there are no other possible outcomes for the coin toss).  If the coin lands heads, you win one dollar.  If the coin lands tails, you win nothing.\r\n\r\nFor a *single* coin toss, the probability you win one dollar is $p$.  The random variable that represents your winnings after one coin toss is a Bernoulli random variable.\r\n\r\nNow, if you toss the coin $5$ times, your winnings could be any whole number of dollars from zero dollars to five dollars, inclusive. The probability that you win five dollars is $p^5$, because each coin toss is independent of the others, and for each coin toss the probability of heads is $p$.\r\n\r\nWhat is the probability that you win *exactly* three dollars in five tosses?  That would require you to toss the coin five times, getting exactly three heads and two tails. This can be achieved with probability $\\binom{5}{3} p^3 (1-p)^2$.  And, in general, if there are $n$ Bernoulli trials, then the sum of those trials is binomially distributed with parameters $n$ and $p$.\r\n\r\nNote that a binomial random variable with parameter $n = 1$ is equivalent to a Bernoulli random variable, i.e. there is only one trial."
    },
    {
        "title": "Combination with repetitions",
        "question": "The formula for computing a k-combination with repetitions from n elements is:\r\n$$\\binom{n + k - 1}{k} = \\binom{n + k - 1}{n - 1}$$\r\n\r\nI would like if someone can give me a simple basic proof that a beginner can understand easily.",
        "answer": "This problem comes by many names - stars and stripes, balls and urns - it&#39;s basically a question of how to distribute $n$ objects (call them &quot;balls&quot;) into $k$ categories (call them &quot;urns&quot;). We can think of it as follows.\r\n\r\nTake $n$ balls and $k-1$ dividers. If a ball falls between two dividers, it goes into the corresponding urn. If there&#39;s nothing between two dividers, then there&#39;s nothing in the corresponding urn. Let&#39;s look at this with a concrete example.\r\n\r\nI want to distribute $5$ balls into $3$ urns. As before, take $5$ balls and $2$ dividers. Visually:\r\n\r\n|ooo|oo\r\n\r\nIn this order, we&#39;d have nothing in the first urn, three in the second urn and two balls in the third urn. The question then is how many ways can we arrange these 5 balls and two dividers? Clearly: $\\dfrac{(5+3-1)!}{5!(3-1)!} = \\displaystyle {7 \\choose 2} = {7 \\choose 5}$.\r\n\r\nWe have that there are $\\dfrac{(n+(k-1))!}{(k-1)! n!}$ the $n$ balls and $k-1$ dividers (since the balls aren&#39;t distinct from each other and the dividers aren&#39;t distinct from each other). Notice that this is equal to $\\displaystyle {n+k-1 \\choose k-1} = {n + k - 1 \\choose n}$. "
    },
    {
        "title": "maximum of two uniform distributions",
        "question": "I have a question. Let&#39;s suppose that the two random variables $X1$ and $X2$ follow two Uniform distributions that are independent but have different parameters:\r\n\r\n$X1 \\sim Uniform(l1, u1)$\r\n\r\n$X2 \\sim Uniform(l2,u2)$\r\n\r\nIf we define X3 as the maximum of X1, and X2, i.e., $X3 = max(X1, X2)$, what kind of distribution would it be? It is certainly not a uniform and I can calculate the cumulative probability, i.e., $p(X3 &lt;= a)$, because $p(X3 &lt;= a) = p(X1 &lt;= a) p(X2 &lt;= a)$. But, I wonder if there exists any density function that can model X3. \r\n\r\n\r\n\r\n\r\n\r\n",
        "answer": "The distribution of $Z = \\max(X,Y)$ of independent random variables is\r\n$$F_Z(z) = P\\{\\max(X,Y)\\leq z\\}\r\n= P\\{X \\leq z, Y \\leq z\\} = P\\{X\\leq z\\}P\\{Y \\leq z\\} = F_X(z)F_y(z)$$\r\nand so the density is\r\n$$f_Z(z) = \\frac{d}{dz}F_Z(z) = f_X(z)F_Y(z) + F_X(z)f_Y(z).$$\r\n\r\nAll of this holds regardless of the distributions of $X$ and $Y$, that is,\r\nthey need not be uniformly distributed random variables. But, for\r\nuniform distributions, the density of $Z$ has simple form since $f_X(z)$ and $f_Y(z)$ are constants and $F_X(z)$ and $F_Y(z)$ are constants or linearly\r\nincreasing functions of $z$.  \r\nFor example, if $X, Y \\sim U(0,1)$, then\r\n$f_Z(z) = \\begin{cases}2z, &amp;0 &lt; z &lt; 1,\\\\\r\n0, &amp;\\text{otherwise},\\end{cases}$ which is not a uniform distribution. On the other hand, if $X\\sim U(a,b)$ and $Y\\sim U(c,d)$ where $c &gt;b$ and so the value of $Y$ is always larger than the value of $X$, then $Z = max(X,Y) = Y$ _is_ uniformly distributed on $(c,d)$. That is, your notion that $\\max(X,Y)$ &quot;is certainly not uniform&quot; should be tempered with a little less certainty.\r\n\r\nExercise: work out the density for the cases when $(a,b)$ and $(c,d)$ overlap _partially_, and when one is a subinterval of the other."
    },
    {
        "title": "Is $\\log X$ integrable when $E(X)=1$?",
        "question": "Let $X$ be a positive valued random variable.\r\n\r\nIf $E(X)=1$, is $E(|\\log X|)$ always finite?",
        "answer": "Not necessarily, take for example a variable $\\tilde{X}$ with density:\r\n$$C \\chi_{(0,\\frac{1}{2})} \\frac{1}{x \\ln(x)^2} dx$$\r\nwhere:\r\n$$\\frac{1}{C} = \\int_0^\\frac{1}{2} \\frac{1}{x \\ln(x)^2} dx &lt;+ \\infty$$\r\n\r\nThen:\r\n$$E(\\tilde{X})=C \\int_0^\\frac{1}{2} \\frac{x}{x \\ln(x)^2} dx &lt;+ \\infty$$\r\nbut:\r\n$$E(\\log(\\tilde{X}))=C \\int_0^\\frac{1}{2} \\frac{\\ln(x)}{x \\ln(x)^2} dx =+ \\infty$$\r\n\r\nwith $X=\\frac{1}{E(\\tilde{X})} \\tilde{X}$ you obtain a counterexample."
    },
    {
        "title": "How does one generally find a joint distribution function (or density) from marginals when there is dependence?",
        "question": "So I know one can go from a joint density function $f(x,y)$ to marginal density functions, like $f_x(x)$ by integrating against the other variables as in $f_x(x) = \\int f(x,y) dy$...but given $f_x(x)$ and $f_y(y)$ as densities for dependent random vars..how would one go about finding a joint density or distribution function?\r\n\r\nThanks",
        "answer": "For example, suppose the marginal densities for $X$ and $Y$ are both 1 on the interval $[0,1]$, 0 otherwise.  One family of possibilities for the joint density is \r\n$f(x,y) = 1 + g(x) h(y)$ for $0 &lt; x &lt; 1$, $0 &lt; y &lt; 1$, 0 otherwise, \r\nfor functions $g$ and $h$ such that $\\int_0^1 g(x)\\, dx = \\int_0^1 h(y)\\, dy = 0$, $-1 \\le g(x) \\le 1$ and $-1 \\le h(y) \\le 1$. And there are infinitely many other possibilities."
    },
    {
        "title": "Finite in probability implies finite expectation",
        "question": "Let $T_n$ be a random variable with $T_n=X_1+...+X_n$ where the $X_i$&#39;s are iid. Further we set $N(t)=max\\{ n: T_n\\leq n\\}$.\r\n\r\nIf $\\Pr(N(t)&lt;\\infty)=1$, does this implies $\\mathbb{E}[N(t)]&lt;\\infty$?\r\n\r\nI think intuitively yes. Since every $N(t)$ is finite, then in the mean it is also finite. Is that correct?\r\n\r\nThank you for any help!!!",
        "answer": "No, it is not correct. Take, for example, $X\\sim\\text{Cauchy}(0,1)$. Then\r\n$$\r\n\\Pr\\{|X|&lt;\\infty\\}=1,\r\n$$\r\nbut $\\operatorname E|X|=\\infty$."
    },
    {
        "title": "Are there relation between $\\mathbb{E}[ \\det(A)]$ and $\\det(\\mathbb{E}[A])$?",
        "question": "Let $A$ be an $n \\times n$ matrix. \r\nIs there a relationship between $\\mathbb{E}[\\det(A)]$ and $\\det(\\mathbb{E}[A])$?\r\n\r\nFor example, for trace we have an equality relationship\r\n\\begin{align}\r\n\\operatorname{Tr}(\\mathbb{E}[A])=\\mathbb{E}[\\operatorname{Tr}(A)]\r\n\\end{align}\r\n",
        "answer": "A simple counter example, \r\nlet $A = \\begin{pmatrix}x&amp;0\\\\0&amp;y\\end{pmatrix}$ \r\n\r\n$\\mathbb{E}\\{\\det A\\}=E\\{xy\\}$  whereas, $\\det \\mathbb{E}\\{A\\}=E\\{x\\}E\\{y\\}$ Unless $x$ and $y$ are independent you can&#39;t claim any relationships."
    },
    {
        "title": "How can I interpret $\\max(X,Y)$?",
        "question": "My textbook says:\r\n\r\n&gt; Let $X$ and $Y$ be two stochastically independent, equally distributed\r\n&gt; random variables with distribution function F. Define $Z = \\max (X, Y)$.\r\n\r\nI don&#39;t understand what is meant by this. I hope I translated it correctly.\r\n\r\nI would conclude that $X=Y$ out of this. And therefore $Z=X=Y$.\r\n\r\nHow can I interpret $\\max(X,Y)$?",
        "answer": "One concrete example:\r\n\r\nSuppose each of the four cells below has probability $1/4$:\r\n$$\r\n\\begin{array}{|c|c|}\r\n\\hline X=0,\\ Y=0 &amp; X=1,\\ Y=0 \\\\  \\hline X=0,\\ Y=1 &amp; X=1,\\ Y=1 \\\\  \\hline\r\n\\end{array}\r\n$$\r\n\r\n\r\nThen here is how $\\max\\{X,Y\\}$ is distributed:\r\n$$\r\n\\begin{array}{|c|c|}\r\n\\hline \\max=0 &amp; \\max=1 \\\\  \\hline \\max=1 &amp; \\max=1 \\\\  \\hline\r\n\\end{array}\r\n$$\r\nEach cell still has probability $1/4$, so $\\Pr(\\max\\{X,Y\\}=1) = 3/4$."
    },
    {
        "title": "If $\\{A, B, C\\}$, $\\{X, Y, Z\\}$, and $\\{A + B + C, X + Y + Z\\}$ are independent random variables, are $\\{A, B, C, X, Y, Z\\}$ independent?",
        "question": "Let {A,B,C} be independent random variables and let {X,Y,Z} be independent random variables. Given that A+B+C and X+Y+Z are independent, is {A,B,C,X,Y,Z} independent? How do I show it?",
        "answer": "No. For a counterexample, consider $A$ and $B$ standard i.i.d. normal, $X=A$, $Y=-B$, and $C=Z=0$. Then:\r\n\r\n- $\\{A,B,C\\}=\\{A,B,0\\}$ is independent\r\n- $\\{X,Y,Z\\}=\\{A,-B,0\\}$ is independent\r\n- $\\{A+B+C,X+Y+Z\\}=\\{A+B,A-B\\}$ is independent\r\n- $\\{A,B,C,X,Y,Z\\}$ is not independent since $B+Y=0$ with $B$ and $Y$ being nondegenerate"
    },
    {
        "title": "$X,Y$ independent then $X+Y$, $X-Y$ independent as well?",
        "question": "My question is simple: If $X$, $Y$ are independent random variables then $X+Y$, $X-Y$ independent as well? ",
        "answer": "Not always.  For example, let, $X$ be the number obtained on rolling a die, and $Y$ the number obtained on rolling another die.  Then clearly $X$ and $Y$ are independent.  However, for example,\r\n$$P(X+Y{=}\\,6\\,\\hbox{and}\\,X-Y{=}\\,1)=0\\ne P(X+Y{=}\\,6)P(X-Y{=}\\,1)\\ ,$$\r\nso $X+Y$ and $X-Y$ are not independent."
    }
]
